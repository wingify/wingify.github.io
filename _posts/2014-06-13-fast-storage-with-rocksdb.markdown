---
layout: post
title: Fast Storage with RocksDB
excerpt: Using RocksDB for persistent fast key-value storage
authorslug: rohit_yadav
author: Rohit Yadav
---

In November last year, I started developing an infrastructure that would allow us to
collect, store, search and retrieve high volume data. The idea was
to collect all the URLs on which our [homegrown CDN](https://visualwebsiteoptimizer.com/split-testing-blog/geo-distributed-architecture/)
would serve JS content. Based on our current traffic, we were looking to collect some 10k URLs per
second across four major geographic regions where we run our servers.

In the beginning we tried MySQL, Redis, Riak, CouchDB, MongoDB, ElasticSearch but
nothing worked out for us with that kind of high speed writes. We also wanted our
system to respond very quickly, under 40ms between
internal servers on private network. This post talks about how we were able to
make such a system using C++11, [RocksDB](http://rocksdb.org) and [Thrift](http://thrift.apache.org).

First, let me start by sharing the use cases of such a system in VWO; the
following screenshot shows a feature where users can enter a URL to check if VWO SmartCode was installed on it.

<div style="text-align:center; margin:5px">
<img src="/images/2014/06/0.png"><br>
<p>VWO SmartCode checker</p>
</div>

The following screenshot shows another feature where users can see a list of URLs
matching a complex wildcard pattern, regex pattern, string rule etc. while
creating a campaign.

<div style="text-align:center; margin:5px">
<img src="/images/2014/06/1.png"><br>
<p>VWO URL Matching Helper</p>
</div>

I [reviewed](http://kkovacs.eu/cassandra-vs-mongodb-vs-couchdb-vs-redis)
several opensource databases but none of them would fit our requirements except
Cassandra. In clustered deployment, reads from Cassandra were too slow and slower
when data size would grew. After understanding how Cassandra worked under the
hood such as its log structured storage like LevelDB I started playing with opensource
embeddable databases that would use similar approach such as LevelDB and Kyoto Cabinet.
At the time, I found an embeddable persistent key-value store
library built on LevelDB called [RocksDB](http://rocksdb.org).
It was opensourced by Facebook and had a fairly active developer community so I
started [playing](https://github.com/facebook/rocksdb/tree/master/examples)
with it. I read the [project wiki](https://github.com/facebook/rocksdb/wiki),
wrote some working code and joined their Facebook group to ask questions around
prefix lookup. The community was helpful, especially Igor and
Siying who gave me [enough hints](https://www.facebook.com/groups/rocksdb.dev/permalink/506160312815821/)
around [prefix lookup](https://github.com/facebook/rocksdb/wiki/Prefix-Seek-API-Changes),
using custom [extractors](https://github.com/facebook/rocksdb/wiki/Hash-based-memtable-implementations)
and [bloom filters](http://en.wikipedia.org/wiki/Bloom_filter) which helped me
write something that actually worked in our production environment for the first time.
Explaining the technology and jargons is out of scope of this post but I would like
to encourage the readers to read
[about](http://google-opensource.blogspot.in/2011/07/leveldb-fast-persistent-key-value-store.html)
[LevelDB](https://code.google.com/p/leveldb/) and to read the RocksDB [wiki](https://github.com/facebook/rocksdb/wiki).

<div style="text-align:center; margin:5px">
<img src="/images/2014/06/2.png"><br>
<p>RocksDB FB Group</p>
</div>

For capturing the URLs with peak velocity up to 10k serves/s, I reused our
[distributed queue based infrastructure](/scaling-with-queues/).
For storage, search and retrieval of URLs I wrote a custom datastore service
using C++, RocksDB and Thrift called _HarvestDB_. [Thrift](http://thrift.apache.org/)
provided the [RPC](http://en.wikipedia.org/wiki/Remote_procedure_call) mechanism
for implementing this system as a distributed service accessible by various
backend sub-systems. The backend sub-systems use client libraries
[generated by Thrift compiler](http://thrift.apache.org/tutorial) for communicating
with the _HarvestDB_ server.

The _HarvestDB_ service implements five remote procedures - ping, get,
put, search and purge. The following [Thrift IDL](http://thrift.apache.org/docs/idl)
describes this service.

```cpp

namespace cpp harvestdb
namespace go harvestdb
namespace py harvestdb
namespace php HarvestDB

struct Url {
    1: required i64    timestamp;
    2: required string url;
    3: required string version;
}

typedef list<Url> UrlList

struct UrlResult {
    1: required i32          prefix;
    2: required i32          found;
    3: required i32          total;
    4: required list<string> urls;
}

service HarvestDB {
    bool ping(),
    Url get(1:i32 prefix, 2:string url),
    bool put(1:i32 prefix, 2:Url url),
    UrlResult search(1:i32 prefix,
                     2:string includeRegex,
                     3:string excludeRegex,
                     4:i32 size,
                     5:i32 timeout),
    bool purge(1:i32 prefix, 2:i64 timestamp)
}
```

Clients use `ping` to check _HarvestDB_ server connectivity before executing
other procedures. RabbitMQ consumers consume collected URLs and `put` them to
_HarvestDB_. The PHP based application backend uses custom Thrift based client
library to `get` (read) and to `search` URLs.
A Python program runs as a periodic cron job and uses `purge` procedure to purge old entries
based on timestamp which makes sure we don't exhaust our storage
resources. The system is in production for more than five months now and is
capable of handling (benchmarked) workload of up to 24k writes/second while consuming
less than 500MB RAM. Our future work will be on replication, sharding and fault
tolerance of this service. The following diagram illustrates this architecture.

<div style="text-align:center; margin:5px">
<img src="/images/2014/06/3.png"><br>
<p>Overall architecture</p>
</div>

[Discussion on Hacker News](https://news.ycombinator.com/item?id=7899353)
