{"componentChunkName":"component---src-templates-post-js","path":"/posts/dynamic-cdn/","result":{"data":{"markdownRemark":{"html":"<p>We, at Wingify, handle not just our own traffic, but also the traffic of\nmajor websites such as <a href=\"http://www.microsoft.com/\">Microsoft</a>, <a href=\"http://www.amd.com/\">AMD</a>, <a href=\"http://www.groupon.com/\">Groupon</a>, and <a href=\"http://www.worldwildlife.org/\">WWF</a> that implement\n<a href=\"https://vwo.com\">Visual Website Optimizer (VWO)</a> for their website optimization. VWO allows\nusers to A/B test their websites and optimize conversions. With an intuitive\nWYSIWYG editor, you can easily make changes to your website and create multiple\nvariations you can A/B test.  When a visitor lands on your website, VWO selects\none of the variations created in the running campaign(s) and the JavaScript\nlibrary does the required modifications to generate the selected variation\nbased on the URL visited seen by the visitor. Furthermore, VWO collects\nanalytics data for every visitor interaction with the website and generates\ndetailed reports to help you understand your audience behavior and provide\ndeeper insight of your business results.</p>\n<p>Here is a very high-level overview of what goes on behind the scenes:</p>\n<div style=\"text-align:center; margin: 5px\">\n\t<img src=\"/images/2014/07/0.png\">\n</div>\n<h2 id=\"how-it-started\" style=\"position:relative;\"><a href=\"#how-it-started\" aria-label=\"how it started permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>How it started</h2>\n<p>Back in the days, we deployed one server in the United States that had the\nstandard LAMP stack running on it. The server stored all changes made to a\nwebsite using VWO app, served our static JS library, collected analytics data,\ncaptured visitor data, and saved it in a MySQL database.</p>\n<div style=\"text-align:center; margin: 5px\">\n\t<img src=\"/images/2014/07/1.png\">\n</div>\n<p>This implementation worked perfectly for us initially, when we were serving a\nlimited number of users. However, as our user base kept growing, we had to\ndeploy additional Load Balancers and Varnish cache servers (each having 32GB\nof RAM and we had 8 such servers to meet our requirements) to make sure that\nwe cache the content for every requested URL and serve back the content in the\nleast possible time.</p>\n<div style=\"text-align:center; margin: 5px\">\n\t<img src=\"/images/2014/07/2.png\">\n</div>\n<p>Gradually, we started using these servers only for serving JS settings and\ncollecting analytics data, and started using Amazon's CloudFront CDN for\nserving static JS library.</p>\n<h2 id=\"issues-we-faced\" style=\"position:relative;\"><a href=\"#issues-we-faced\" aria-label=\"issues we faced permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Issues we faced</h2>\n<p>This worked great for a while till we hit our traffic to more than 1k requests\nper sec. With so much of traffic coming in and the increasing number of unique\nURLs being tested, the system started failing. We experienced frequent cache\nmisses and Varnish required more RAM to cope up with the new requirements. We\nknew we had hit the bottom-end there and quickly realized that it was time for\nus to stop everything and get our thinking caps back on to redesign the\narchitecture. We now needed a scalable system that was easier to maintain, and\nwould cater to the needs of our users from various geo locations.</p>\n<h2 id=\"the-new-requirements\" style=\"position:relative;\"><a href=\"#the-new-requirements\" aria-label=\"the new requirements permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>The new requirements</h2>\n<p>Today, VWO uses a Dynamic CDN built in-house that can cater to users based in\nany part of the world. The current implementation offers us with the following\nadvantages in comparison with other available CDNs:</p>\n<ul>\n<li>Capability of handling almost any amount of requests at average response\ntimes of 50ms</li>\n<li>Handles 10k+ request/sec per node (8GB RAM). We have benchmarked this system\nto handle 50k requests/sec per node in our current production scenario</li>\n<li>100% uptime</li>\n<li>Improved response time and data acquisition as the servers are closer to the\nuser, thus minimizing the latency and increasing the chances of successful\ndelivery of data</li>\n<li>Considerable cost savings as compared to the previous system</li>\n<li>Freedom to add new nodes without any dependencies on other nodes</li>\n</ul>\n<h3 id=\"implementation-challenges-and-technicalities\" style=\"position:relative;\"><a href=\"#implementation-challenges-and-technicalities\" aria-label=\"implementation challenges and technicalities permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Implementation challenges and technicalities</h3>\n<p>The core issue we had to resolve was to avoid sending the same response for all\nthe requests coming from a domain or a particular account. In the old\nimplementation, we were serving JSON for all the campaigns running in an\naccount, irrespective of a campaign running on that URL. This loaded\nunnecessary JS code, which might not be useful for a particular URL, thereby\nincreasing load time of the website. We knew how page-load time is crucial for\nonline businesses and how it directly impacts their revenue. In the marketing\nworld, the users are less likely to make purchases from a slow loading website\nas compared to a fast loading website.</p>\n<div style=\"text-align:center; margin:5px\">\n\t<img src=\"/images/2014/07/3.png\">\n</div>\n<p>It is important to make sure that we only serve relevant content based on the\nURL of the page. There are two ways to do this:</p>\n<ul>\n<li>Cache JSON for all the URLs and use cache like Varnish (the old system).</li>\n<li>Cache each campaign running in an account and then build/combine the settings\ndynamically for each URL. This approach is the fastest possible way of\nimplementation with least amount of resources.</li>\n</ul>\n<p>With the approach identified, we started looking for nodes that could do\neverything for us - generate dynamic JSON on the basis of request, serve static\nJS library, and handle data acquisition. Another challenge was to make these\nnodes a part of distributed system that spreads across different geographies,\nwith no dependency on each other while making sure that the request is served\nfrom the closest location instead of nodes only in the US. We had written a\nblog post earlier to explain this to our customers. <a href=\"https://vwo.com/blog/geo-distributed-architecture/\">Read it here</a>.</p>\n<div style=\"text-align:center; margin:5px\">\n\t<img src=\"/images/2014/07/4.png\">\n</div>\n<p><a href=\"http://openresty.org/\">OpenResty (aka. ngx_openresty)</a> our current workhorse, is a full-fledged web\napplication server created by bundling the standard Nginx core with different\n<a href=\"http://wiki.nginx.org/3rdPartyModules\">3rd-party Nginx modules</a> and their external dependencies. It also bundles Lua\nmodules to allow writing URL handlers in Lua, and the Lua code runs within the\nweb server.</p>\n<p>From 1 server running Apache + PHP to multiple nodes involving Nginx (load\nbalancer) -> Varnish (cache) -> Apache + PHP (for cache miss + data\ncollection), to the current system where each node in itself is capable of\nhandling all types of requests. We serve our static JS library, JSON settings\nfor every campaign and also use these servers for analytics data acquisition.</p>\n<p>The following section describes briefly the new architecture of our CDN and how\nVWO servers handle requests:</p>\n<div style=\"text-align:center; margin:5px\">\n\t<img src=\"/images/2014/07/5.png\">\n</div>\n<ol>\n<li>We use <a href=\"http://wiki.nginx.org/HttpLuaModule#ngx.shared.DICT\">Nginx-Lua Shared Dictionary</a>, an in-memory\nstore shared among all the Nginx worker processes to store campaign specific\ndata. Memcached is used as the first fallback if we have to restart the\nOpenResty server (it resets the shared dictionary). Our second fallback is\nour central MySQL database. If any request fails at any level, [the system]\nfetches it from the lower layer and responses are saved in all the above\nlevels to make them available for the next request.</li>\n<li>Once the request hits our server to fetch JSON for the campaigns running on\na webpage, VWO runs a regex match for the requested URL with the list of URL\nregex patterns stored in the Nginx-Lua shared dictionary (key being Account\nID, O(1) lookup, FAST!). This returns the list of campaign IDs valid for the\nrequested URL. All the regex patterns are compiled and cached in\nworker-process level and shared among all requests.</li>\n<li>Next, VWO looks up for the campaign IDs (returned after matching the\nrequested URL) in the Nginx-Lua shared dictionary, with Account ID and\nCampaign ID as key (again an O(1) lookup). This returns the settings for all\ncampaigns, which are then combined and sent with some additional data in\nresponse based on requests such as geo-location data, 3rd party integrations\nspecific code, etc. We ensure that the caching layer does not have stale\ndata and is updated within a few milliseconds. This offers us advantage in\nterms of validation time taken by most CDNs available.</li>\n<li>To ensure that the request is served from the closest server to the visitor,\nwe use managed DNS services from DynECT that keeps a check on the response\ntimes from various POPs and replies with the best possible server IPs (both\nin terms of health and distance). This helps us ensure a failsafe delivery\nnetwork.</li>\n<li>To ensure that the system captures analytics data, all data related to\nvisitors, conversions and heatmaps is sent to these servers. We use\nOpenresty with Lua for collecting all incoming data. All the data received\nat Openresty end is pushed to a Redis server running on all these machines.\nThe Redis server writes the data as fast as possible, thereby reducing the\nchance of data loss. Next, we move data from the Redis servers to central\nRabbitMQ. This incoming data is then used by multiple consumers in various\nways and stored at multiple places for different purposes. You can check our\nprevious post <a href=\"http://engineering.wingify.com/scaling-with-queues/\">Scaling with Queues</a> to understand more about our data\nacquisition setup.</li>\n</ol>\n<p>As our customers keep growing and our traffic keeps growing, we will be able to\njudge better about our system, how well it scales and what problems it has. And\nas VWO grows and becomes a better and better, we will keep working on our\ncurrent infrastructure to improve it and adjust it for our needs. We would like\nto thank <a href=\"http://agentzh.org/\">agentzh (YichunZhang)</a> for building OpenResty and for helping us\nout whenever we were stuck with our implementation.</p>\n<p>We work in a dynamic environment where we collaborate and work towards\narchitecting scalable and fault-tolerant systems like these. If these kind of\nproblems and challenges interest you, we will be happy to work with you. <a href=\"https://wingify.com/careers\">We\nare hiring!</a></p>","timeToRead":6,"excerpt":"We, at Wingify, handle not just our own traffic, but also the traffic of\nmajor websites such as Microsoft, AMD, Groupon, and WWF that…","frontmatter":{"title":"Dynamic CDN","author":"Ankit Jain","authorslug":"ankit_jain"},"fields":{"slug":"/posts/dynamic-cdn/","date":"July 22, 2014"}}},"pageContext":{"slug":"/posts/dynamic-cdn/","date":"2014-07-22T18:30:00.000Z","nexttitle":"We are sponsoring The Fifth Elephant 2014","nextslug":"/posts/sponsoring-fifth-elephant/","prevtitle":"Wingify at The Fifth Elephant","prevslug":"/posts/wingify-at-the-fifth-elephant/"}}}