{"componentChunkName":"component---src-templates-post-js","path":"/posts/fast-storage-with-rocksdb/","result":{"data":{"markdownRemark":{"html":"<p>In November last year, I started developing an infrastructure that would allow us to\ncollect, store, search and retrieve high volume data. The idea was\nto collect all the URLs on which our <a href=\"https://visualwebsiteoptimizer.com/split-testing-blog/geo-distributed-architecture/\">homegrown CDN</a>\nwould serve JS content. Based on our current traffic, we were looking to collect some 10k URLs per\nsecond across four major geographic regions where we run our servers.</p>\n<p>In the beginning we tried MySQL, Redis, Riak, CouchDB, MongoDB, ElasticSearch but\nnothing worked out for us with that kind of high speed writes. We also wanted our\nsystem to respond very quickly, under 40ms between\ninternal servers on private network. This post talks about how we were able to\nmake such a system using C++11, <a href=\"http://rocksdb.org\">RocksDB</a> and <a href=\"http://thrift.apache.org\">Thrift</a>.</p>\n<p>First, let me start by sharing the use cases of such a system in VWO; the\nfollowing screenshot shows a feature where users can enter a URL to check if VWO\nSmart Code was installed on it.</p>\n<div style=\"text-align:center; margin:5px\">\n<img src=\"/images/2014/06/0.png\"><br>\n<p>VWO Smart Code checker</p>\n</div>\n<p>The following screenshot shows another feature where users can see a list of URLs\nmatching a complex wildcard pattern, regex pattern, string rule etc. while\ncreating a campaign.</p>\n<div style=\"text-align:center; margin:5px\">\n<img src=\"/images/2014/06/1.png\"><br>\n<p>VWO URL Matching Helper</p>\n</div>\n<p>I <a href=\"http://kkovacs.eu/cassandra-vs-mongodb-vs-couchdb-vs-redis\">reviewed</a>\nseveral opensource databases but none of them would fit our requirements except\nCassandra. In clustered deployment, reads from Cassandra were too slow and slower\nwhen data size would grew. After understanding how Cassandra worked under the\nhood such as its log structured storage like LevelDB I started playing with opensource\nembeddable databases that would use similar approach such as LevelDB and Kyoto Cabinet.\nAt the time, I found an embeddable persistent key-value store\nlibrary built on LevelDB called <a href=\"http://rocksdb.org\">RocksDB</a>.\nIt was opensourced by Facebook and had a fairly active developer community so I\nstarted <a href=\"https://github.com/facebook/rocksdb/tree/master/examples\">playing</a>\nwith it. I read the <a href=\"https://github.com/facebook/rocksdb/wiki\">project wiki</a>,\nwrote some working code and joined their Facebook group to ask questions around\nprefix lookup. The community was helpful, especially Igor and\nSiying who gave me <a href=\"https://www.facebook.com/groups/rocksdb.dev/permalink/506160312815821/\">enough hints</a>\naround <a href=\"https://github.com/facebook/rocksdb/wiki/Prefix-Seek-API-Changes\">prefix lookup</a>,\nusing custom <a href=\"https://github.com/facebook/rocksdb/wiki/Hash-based-memtable-implementations\">extractors</a>\nand <a href=\"http://en.wikipedia.org/wiki/Bloom_filter\">bloom filters</a> which helped me\nwrite something that actually worked in our production environment for the first time.\nExplaining the technology and jargons is out of scope of this post but I would like\nto encourage the readers to read\n<a href=\"http://google-opensource.blogspot.in/2011/07/leveldb-fast-persistent-key-value-store.html\">about</a>\n<a href=\"https://code.google.com/p/leveldb/\">LevelDB</a> and to read the RocksDB <a href=\"https://github.com/facebook/rocksdb/wiki\">wiki</a>.</p>\n<div style=\"text-align:center; margin:5px\">\n<img src=\"/images/2014/06/2.png\"><br>\n<p>RocksDB FB Group</p>\n</div>\n<p>For capturing the URLs with peak velocity up to 10k serves/s, I reused our\n<a href=\"/scaling-with-queues/\">distributed queue based infrastructure</a>.\nFor storage, search and retrieval of URLs I wrote a custom datastore service\nusing C++, RocksDB and Thrift called <em>HarvestDB</em>. <a href=\"http://thrift.apache.org/\">Thrift</a>\nprovided the <a href=\"http://en.wikipedia.org/wiki/Remote_procedure_call\">RPC</a> mechanism\nfor implementing this system as a distributed service accessible by various\nbackend sub-systems. The backend sub-systems use client libraries\n<a href=\"http://thrift.apache.org/tutorial\">generated by Thrift compiler</a> for communicating\nwith the <em>HarvestDB</em> server.</p>\n<p>The <em>HarvestDB</em> service implements five remote procedures - ping, get,\nput, search and purge. The following <a href=\"http://thrift.apache.org/docs/idl\">Thrift IDL</a>\ndescribes this service.</p>\n<p>{% highlight cpp %}</p>\n<p>namespace cpp harvestdb\nnamespace go harvestdb\nnamespace py harvestdb\nnamespace php HarvestDB</p>\n<p>struct Url {\n1: required i64    timestamp;\n2: required string url;\n3: required string version;\n}</p>\n<p>typedef list<Url> UrlList</p>\n<p>struct UrlResult {\n1: required i32          prefix;\n2: required i32          found;\n3: required i32          total;\n4: required list<string> urls;\n}</p>\n<p>service HarvestDB {\nbool ping(),\nUrl get(1:i32 prefix, 2:string url),\nbool put(1:i32 prefix, 2:Url url),\nUrlResult search(1:i32 prefix,\n2:string includeRegex,\n3:string excludeRegex,\n4:i32 size,\n5:i32 timeout),\nbool purge(1:i32 prefix, 2:i64 timestamp)\n}\n{% endhighlight %}</p>\n<p>Clients use <code class=\"language-text\">ping</code> to check <em>HarvestDB</em> server connectivity before executing\nother procedures. RabbitMQ consumers consume collected URLs and <code class=\"language-text\">put</code> them to\n<em>HarvestDB</em>. The PHP based application backend uses custom Thrift based client\nlibrary to <code class=\"language-text\">get</code> (read) and to <code class=\"language-text\">search</code> URLs.\nA Python program runs as a periodic cron job and uses <code class=\"language-text\">purge</code> procedure to purge old entries\nbased on timestamp which makes sure we don't exhaust our storage\nresources. The system is in production for more than five months now and is\ncapable of handling (benchmarked) workload of up to 24k writes/second while consuming\nless than 500MB RAM. Our future work will be on replication, sharding and fault\ntolerance of this service. The following diagram illustrates this architecture.</p>\n<div style=\"text-align:center; margin:5px\">\n<img src=\"/images/2014/06/3.png\"><br>\n<p>Overall architecture</p>\n</div>\n<p><a href=\"https://news.ycombinator.com/item?id=7899353\">Discussion on Hacker News</a></p>","timeToRead":4,"excerpt":"In November last year, I started developing an infrastructure that would allow us to\ncollect, store, search and retrieve high volume dataâ€¦","frontmatter":{"title":"Fast Storage with RocksDB","author":"Rohit Yadav","authorslug":"rohit_yadav"},"fields":{"slug":"/posts/fast-storage-with-rocksdb/","date":"June 12, 2014"}}},"pageContext":{"slug":"/posts/fast-storage-with-rocksdb/","date":"2014-06-12T18:30:00.000Z","nexttitle":"Automated e2e testing- WebDriverJS, Jasmine and Protractor","nextslug":"/posts/e2e-testing-with-webdriverjs-jasmine/","prevtitle":"We are sponsoring The Fifth Elephant 2014","prevslug":"/posts/sponsoring-fifth-elephant/"}}}