{"componentChunkName":"component---src-templates-post-js","path":"/posts/wingify-pydata-at-wingify/","result":{"data":{"markdownRemark":{"html":"<img src=\"/images/2017/06/pydata_6.jpg\">\n<h3 id=\"about-pydata\" style=\"position:relative;\"><a href=\"#about-pydata\" aria-label=\"about pydata permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>About PyData</h3>\n<p>I recently got an opportunity to speak at the PyData, Delhi. PyData is a tech group, with chapters in New Delhi and other regions, where Python enthusiasts share their ideas and projects related to Data Analysis and Machine Learning.</p>\n<h3 id=\"talks-at-pydata\" style=\"position:relative;\"><a href=\"#talks-at-pydata\" aria-label=\"talks at pydata permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Talks at PyData</h3>\n<p>There were three talks at PyData, namely <strong>Machine Learning using Tensor Flow</strong>, <strong>Data Layer at Wingify</strong> and mine, <strong>Learning Data Analysis by Scraping Websites</strong>. All the talks were thorough and excellent! In the talk, <strong>Data layer at Wingify</strong> by <a href=\"https://twitter.com/mgill25\">Manish Gill</a> ü§ì, he talked about how we handle millions of requests at Wingify.</p>\n<h3 id=\"some-of-images-of-the-pydata-meetup-hosted-by-wingify\" style=\"position:relative;\"><a href=\"#some-of-images-of-the-pydata-meetup-hosted-by-wingify\" aria-label=\"some of images of the pydata meetup hosted by wingify permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Some of Images of the PyData Meetup Hosted by Wingify.</h3>\n<script>Galleria.run('#pydata-meetup-gallery');</script>\n<div id=\"pydata-meetup-gallery\" style=\"height: 600px;\">\n    <img src=\"/images/2017/06/pydata_0.jpg\">\n    <img src=\"/images/2017/06/pydata_8.jpg\">\n    <img src=\"/images/2017/06/pydata_7.jpg\">\n    <img src=\"/images/2017/06/pydata_2.jpg\">\n    <img src=\"/images/2017/06/pydata_1.jpg\">\n    <img src=\"/images/2017/06/pydata_5.jpg\">\n    <img src=\"/images/2017/06/pydata_4.jpg\">\n    <img src=\"/images/2017/06/pydata_9.jpg\">\n</div>\n<h3 id=\"background-about-my-talk\" style=\"position:relative;\"><a href=\"#background-about-my-talk\" aria-label=\"background about my talk permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Background About My Talk</h3>\n<p>Let me give you a little background. It was the Friday before the PyData Meetup/Conference. Our engineering team was doing its daily tasks. I had just grabbed a coffee to alleviate my laziness. Suddenly, our engineering lead came and asked us whether anyone could present on a topic at the PyData that we were to organise the very next day. An initial speaker, who had confirmed earlier, backed out at the last moment because he had fallen sick. I could see that most of the team members tried to avoid volunteering in such a short notice and also probably because the next day was a Saturday (though this is my personal opinion). But I had something different on my mind and during this planning or confusion, I volunteered for it ü§ì. I had a project that I had done, back when I was learning Python. So I offered to present it. He agreed to it and asked me to keep the presentation ready.</p>\n<h3 id=\"preparing-the-project--slides\" style=\"position:relative;\"><a href=\"#preparing-the-project--slides\" aria-label=\"preparing the project  slides permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Preparing the Project &#x26; Slides</h3>\n<p>That Friday night, I started searching for the old files which I had used. Finally, I found all of them on my website, downloaded them and ran the code. It worked like a charm üòç. Yeah! I quickly created the slides around it, and after finishing, smiled and went to sleep at 4.30 am.</p>\n<h3 id=\"little-about-the-basics-of-my-talk\" style=\"position:relative;\"><a href=\"#little-about-the-basics-of-my-talk\" aria-label=\"little about the basics of my talk permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Little About the Basics of My Talk.</h3>\n<p>The presentation that I gave was on <strong>Learning Data Analysis by Scraping Websites</strong>. During my college days, we heavily used the BeautifulSoup Library in Python to scrape websites for the many personal projects. During this project, I got the idea to scrape data from the websites which aggregated movies related data. By doing that, I thought that I could create a list of all movies that I must definitely watch. The movies had to satisfy the following criteria:</p>\n<ol>\n<li>Release date >= 2000</li>\n<li>Rating > 8</li>\n</ol>\n<p>It was not the best idea at that time to scrape websites and then analyse(Data frame). But I learned a lot of things by scraping data from the website using Beautifulsoup, then analyzing data using Pandas, visualizing data using MatplotLib (a Python library) and finally coming to conclusion about my movies recommendation.</p>\n<p>Coming back to the objective - <code class=\"language-text\">Finding and sorting the movies released between 2000-2017 in the order of relevance</code> (I didn't want to watch movies &#x3C; 2000).\nBelow is the code to scrape <a href=\"http://www.imdb.com/\">IMDB</a> for movies data from 2000-2017.</p>\n<p>{% highlight js %}\nfrom bs4 import BeautifulSoup\nimport urllib2\ndef main():\nprint(\"** ======  Data Extracting Lib -- by Promode  ===== **\")\ntestUrl = \"<a href=\"http://www.imdb.com/search/title?at=0&#x26;count=100&#x26;%5C\">http://www.imdb.com/search/title?at=0&#x26;count=100&#x26;\\</a>\ngroups=top<em>1000&#x26;release</em>date=2000,2017&#x26;sort=moviemeter\"\npageSource = urllib2.urlopen(testUrl).read()\nsoupPKG = BeautifulSoup(pageSource, 'lxml')\ntitles = soupPKG.findAll(\"div\",class_='lister-item mode-advanced')\nmymovieslist = []\nmymovies = {}\nfor t in titles:\nmymovies = {}\nmymovies['name'] = t.findAll(\"a\")[1].text\nmymovies['year'] = str(t.find(\"span\", \"lister-item-year\").text).replace('','')\nmymovies['rating'] = float(str(t.find(\"span\", \"rating-rating\").text)<br>\n.replace('','')[0:-3])\nmymovies['runtime'] = t.find(\"span\", \"runtime\").text\nmymovieslist.append(mymovies)\nprint mymovieslist\nif <strong>name</strong>==\"<strong>main</strong>\":\nmain()\n{% endhighlight %}</p>\n<p><a href=\"https://github.com/PramodDutta/ScrapToDataAnalysis\">Click here</a> to have a look at the full source code.</p>\n<p>You can see the trends like  <code class=\"language-text\">Maximum Rating - Sorted by Rating</code>, <code class=\"language-text\">Year Vs Rating Trend</code></p>\n<div style=\"text-align:center; margin: 10px;\">\n  <img src=\"/images/2017/06/pydata_1_M.png\" style=\"box-shadow: 2px 2px 10px 1px #aaa\">\n  <div style=\"margin: 10px;\"><b>DataFrame  - Rating is Set as Index</b></div>\n</div>\n<br>\n<div style=\"text-align:center; margin: 10px;\">\n  <img src=\"/images/2017/06/pydata_tt.png\" style=\"box-shadow: 2px 2px 10px 1px #aaa\">\n  <div style=\"margin: 10px;\"><b>Maximum Rating - Sorted by Rating</b></div>\n</div>\n<br>\n<div style=\"text-align:center; margin: 10px;\">\n  <img src=\"/images/2017/06/pydata_graph.png\" style=\"box-shadow: 2px 2px 10px 1px #aaa\">\n  <div style=\"margin: 10px;\"><b>Year Vs Rating Trend</b></div>\n</div>\n<h3 id=\"take-away-from-the-talk\" style=\"position:relative;\"><a href=\"#take-away-from-the-talk\" aria-label=\"take away from the talk permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Take away from the Talk</h3>\n<p>With this method, you would have winner's data from the data set. For example, suppose you want to create a Cricket Team(IPLT20) which has the maximum probability to win the match, what you can do is parse the <a href=\"http://www.iplt20.com/\">IPLT20</a>) website for last 5 years' data and select the top 5 batsmen and 6 bowlers üòé.</p>\n<h3 id=\"conclusion\" style=\"position:relative;\"><a href=\"#conclusion\" aria-label=\"conclusion permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Conclusion</h3>\n<p>I totally understand that this may not be the best project for the data analysis. I am still learning and I showed what I had done. I believe that it served my purpose.</p>\n<p>I will be doing more research on data analysis in Python. Thanks for reading this.\nBelow is my talk slides:</p>\n<h3 id=\"slides\" style=\"position:relative;\"><a href=\"#slides\" aria-label=\"slides permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Slides:</h3>\n<p>Slides :- </p>\n<iframe src=\"//slides.com/pramoddutta-1/deck/embed\" width=\"100%\" height=\"600px\" scrolling=\"no\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>","timeToRead":4,"excerpt":"About PyData I recently got an opportunity to speak at the PyData, Delhi. PyData is a tech group, with chapters in New Delhi and other‚Ä¶","frontmatter":{"title":"PyData at Wingify - My Experience","author":"Pramod Dutta","authorslug":"pramod_dutta"},"fields":{"slug":"/posts/wingify-pydata-at-wingify/","date":"September 11, 2017"}}},"pageContext":{"slug":"/posts/wingify-pydata-at-wingify/","date":"2017-09-11T18:30:00.000Z","nexttitle":"Demand-driven APIs Using GraphQL","nextslug":"/posts/demand-drive-apis-using-graphql/","prevtitle":"Automated environment deployments","prevslug":"/posts/automated-environment-deployments/"}}}